сайт
https://kp-opt.ru

доработка на странице
https://kp-opt.ru/issledovatel/

расположение на виртуальном хостинге таимвеб
/home/c/ch38922/bitrix_yl1dy/public_html/issledovatel


# Исследователь AI

Анализ прайс-листов с использованием искусственного интеллекта.

## Описание проекта

"Исследователь AI" - это веб-приложение, предназначенное для интеллектуального анализа прайс-листов поставщиков. Система использует AI-модели (OpenAI, DeepSeek) для обработки запросов пользователей на естественном языке, поиска релевантной информации в загруженных прайс-листах и предоставления структурированных ответов. Интеграция с Яндекс.Диском позволяет автоматически синхронизировать и обрабатывать файлы прайс-листов. Приложение интегрировано в существующую CMS Bitrix.

## Ключевые технологии

*   **Бэкенд:** PHP
*   **Фреймворк/CMS:** Интеграция с Bitrix (для UI и пользовательской сессии)
*   **База данных:** SQLite (для кэширования данных, хранения истории чатов, настроек, и векторных представлений)
*   **AI Провайдеры:** OpenAI (GPT-модели), DeepSeek
*   **Хранение файлов:** Яндекс.Диск
*   **Управление зависимостями:** Composer
*   **Фронтенд:** HTML, CSS, JavaScript (с использованием Bootstrap 5)

## Структура проекта

Ниже представлена предполагаемая структура директорий и ключевых файлов проекта:

/
├── api/                     # API эндпоинты для AJAX запросов
│   ├── check_status.php     # Проверка статуса API и внешних сервисов (OpenAI, Yandex Disk)
│   ├── create_chat.php      # Создание нового чата в базе данных
│   ├── get_chat.php         # Получение истории сообщений для конкретного чата
│   ├── get_chats.php        # Получение списка всех чатов пользователя
│   ├── process_query.php    # Основной эндпоинт для обработки запроса пользователя к AI
│   ├── save_message.php     # Сохранение сообщения пользователя или AI в базу данных
│   └── save_settings.php    # Сохранение настроек приложения (API ключи, пути и т.д.)
├── classes/                 # PHP классы (PSR-4, неймспейс ResearcherAI)
│   ├── AIProvider.php       # (Возможный) Интерфейс или абстрактный класс для AI провайдеров
│   ├── AIProviderFactory.php # Фабрика для создания экземпляров AI провайдеров (OpenAI, DeepSeek)
│   ├── CacheManager.php     # Управление кэшем SQLite (тексты файлов, метаданные, в будущем - векторы)
│   ├── DeepSeekProvider.php # Реализация AI провайдера для DeepSeek API
│   ├── OpenAIProvider.php   # Реализация AI провайдера для OpenAI API
│   ├── PriceAnalyzer.php    # Класс для анализа и структурирования информации из прайс-листов
│   ├── TextExtractor.php    # (Предполагаемый) Класс для извлечения текста из различных форматов файлов (xlsx, docx, pdf)
│   └── YandexDiskClient.php # Клиент для взаимодействия с API Яндекс.Диска (загрузка файлов, получение списка)
├── config/                  # Файлы конфигурации
│   └── database.php         # Настройка и инициализация подключения к БД SQLite
├── vendor/                  # Зависимости, управляемые Composer
├── .htaccess                # (Возможный) Файл конфигурации Apache для ЧПУ и других настроек
├── composer.json            # Файл конфигурации Composer, описывающий зависимости и автозагрузку
├── composer.lock            # Lock-файл Composer с точными версиями установленных пакетов
├── composer.phar            # Исполняемый файл Composer (если используется локально)
├── cache.sqlite             # Файл базы данных SQLite
├── index.php                # Главная страница приложения, интегрированная в Bitrix (пользовательский интерфейс чата)
└── README.md                # Этот файл с описанием проекта
Установка и запуск
Клонирование репозитория: Если проект находится под версионным контролем Git:
bash
CopyInsert
git clone <URL_репозитория>
cd issledovatel
Если нет, убедитесь, что все файлы проекта находятся в директории D:\Pycharm\issledovatel (или аналогичной на сервере).
Установка зависимостей Composer: Убедитесь, что composer.phar находится в корне проекта или Composer установлен глобально. Из корневой директории проекта выполните:
bash
CopyInsert in Terminal
php composer.phar install
Или, если Composer установлен глобально:
bash
CopyInsert in Terminal
composer install
Эта команда установит все необходимые библиотеки, указанные в composer.json, в папку vendor/.
Настройка веб-сервера: Проект предназначен для работы в окружении Bitrix. Убедитесь, что ваш веб-сервер (например, Apache или Nginx) корректно настроен для работы с Bitrix, и папка /issledovatel/ доступна по URL (например, https://kp-opt.ru/issledovatel/).
База данных SQLite: Файл cache.sqlite должен быть создан и доступен для записи PHP-скриптами. При первом запуске или через механизм миграций (если он будет реализован) должны быть созданы необходимые таблицы: researcher_settings, chats, messages, и в будущем таблицы для векторов и индексированных файлов. Файл config/database.php отвечает за подключение к этой базе.
Интеграция с Bitrix: Главный файл index.php использует стандартные вызовы Bitrix для подключения хедера и футера:
php
CopyInsert
require($_SERVER["DOCUMENT_ROOT"]."/bitrix/header.php");
// ... код страницы ...
require($_SERVER["DOCUMENT_ROOT"]."/bitrix/footer.php");
Пользователь должен быть авторизован в Bitrix и иметь права администратора для доступа к странице.
Настройка приложения:
Откройте страницу приложения в браузере (например, https://kp-opt.ru/issledovatel/).
Используйте интерфейс настроек (иконка шестеренки) для ввода:
API ключей для OpenAI и/или DeepSeek.
OAuth токена для Яндекс.Диска.
Пути к папке с прайс-листами на Яндекс.Диске.
Настроек прокси, если это необходимо для доступа к API OpenAI.
План развития
Этап 1: Создание векторной базы и базовый RAG (Почти завершён)
Парсинг и Векторизация:
Использование существующего механизма извлечения текста из файлов (например, из .xlsx, .docx, .pdf с помощью соответствующих библиотек).
Внедрение векторизации извлеченного текста с помощью доступных embedding-моделей (например, OpenAI text-embedding-ada-002 или аналоги от DeepSeek).
Разработка или расширение структуры таблиц в cache.sqlite для хранения:
indexed_files: информация об обработанных файлах (путь на Яндекс.Диске, хэш содержимого, дата последней модификации на Диске, дата последней индексации).
file_vectors (или аналогичная таблица): хранение векторов текстовых фрагментов (чанков), сами текстовые фрагменты, связь с indexed_files, метаданные чанка.
Отслеживание изменений файлов:
Модификация YandexDiskClient для получения списка файлов и их метаданных (включая хэши или даты изменения, если API Диска их предоставляет).
Реализация в CacheManager (или специализированном VectorStoreManager) логики сравнения текущего состояния файлов на Диске с записями в indexed_files для выявления новых, измененных или удаленных файлов.
Автоматический запуск пере-парсинга и пере-векторизации для измененных/новых файлов и удаление данных для удаленных файлов.
Базовый RAG (Retrieval Augmented Generation):
Запрос пользователя векторизуется с использованием той же embedding-модели.
Выполняется поиск K наиболее семантически близких векторов (и соответствующих им текстовых фрагментов) в локальной базе file_vectors (например, с использованием косинусного сходства).
Найденные релевантные текстовые фрагменты (контекст) вместе с исходным запросом пользователя передаются в выбранную LLM (OpenAI/DeepSeek) для генерации ответа.
В ответе пользователю указываются источники информации (например, имена файлов, из которых был взят контекст).
Текущий статус: Основные компоненты для этого этапа частично реализованы или находятся в процессе завершения.
Цель этапа: Получить работающую систему семантического поиска по содержимому прайс-листов с возможностью актуализации данных при изменении файлов на Яндекс.Диске.
Этап 2: Улучшенный парсинг с GPT и отслеживание структуры папок
GPT-парсинг (Интеллектуальное извлечение данных):
Разработка механизма, где вместо простого извлечения "сырого" текста, содержимое файла (или его значимые части) передается в продвинутую LLM (например, GPT-4) со специально составленным промптом. Промпт будет нацелен на извлечение структурированной информации: наименования товаров, цены, артикулы, характеристики, условия поставки и т.д.
Результат GPT-парсинга (например, JSON) будет использоваться для создания более качественных и осмысленных текстовых представлений для векторизации. Это могут быть как отдельные структурированные записи, так и обобщенные описания товаров/услуг, сгенерированные GPT.
Обновление таблиц для хранения векторов для поддержки этих структурированных или обогащенных данных.
Отслеживание изменений структуры папок:
Доработка YandexDiskClient для рекурсивного сканирования указанной корневой папки на Яндекс.Диске, включая все вложенные папки.
Реализация отслеживания добавления/удаления/переименования не только файлов, но и самих папок.
Обновление логики в CacheManager / VectorStoreManager для корректной обработки изменений в иерархии папок. Это включает обновление путей к файлам, удаление данных, связанных с удаленными папками, и индексацию файлов в новых папках.
Цель этапа: Значительно повысить качество и релевантность извлекаемых данных для векторизации за счет интеллектуального парсинга. Обеспечить полную и точную синхронизацию с любой структурой папок (включая вложенные) на Яндекс.Диске.
Этап 3: Оптимизация, масштабирование и UI/UX улучшения
Оптимизация хранения и поиска векторов:
Проведение анализа производительности поиска векторов в SQLite при увеличении их количества.
Если стандартный поиск (например, полный перебор и вычисление косинусного сходства "на лету") становится узким местом, рассмотреть следующие шаги:
Внедрение специализированных расширений для SQLite, поддерживающих эффективный векторный поиск (ANN - Approximate Nearest Neighbor), например, sqlite-vss.
Как альтернатива (при очень больших объемах данных и высоких требованиях к производительности): рассмотреть миграцию векторных данных в специализированную векторную базу данных (например, Qdrant, Weaviate, Pinecone). Это усложнит архитектуру и процесс развертывания, но может предоставить существенный прирост производительности и дополнительные возможности для работы с векторами.
Фоновая обработка / Очереди задач:
Для объемных прайс-листов или большого количества файлов процессы GPT-парсинга и векторизации могут занимать значительное время, что может приводить к блокировке ответа API или таймаутам HTTP-запросов.
Рассмотреть вынесение этих длительных задач в фоновый режим. Возможные реализации:
Простая система на основе cron-задач, которая периодически проверяет наличие новых/измененных файлов и запускает их обработку.
Более надежная система с использованием очередей задач (например, с помощью RabbitMQ, Redis queues, или даже специальной таблицы задач в SQLite). API будет добавлять задачу на индексацию в очередь, а отдельный worker-скрипт (запускаемый по cron или работающий как демон) будет разбирать очередь и выполнять задачи асинхронно.
API должен быстро отвечать пользователю (например, "Файл успешно добавлен в очередь на обработку"), а сама индексация будет происходить в фоновом режиме.
Улучшения UI/UX:
Добавить в пользовательский интерфейс наглядную индикацию процесса индексации файлов (например, прогресс-бар, статус "Идет индексация N из M файлов...", отображение времени последней успешной синхронизации).
Улучшить отображение источников информации в ответах AI. Если GPT-парсинг (Этап 2) позволит более точно связывать сгенерированный ответ с конкретными товарами, разделами или даже строками в прайс-листах, это должно быть отражено в UI (например, не просто имя файла, а "Товар X из файла Y.xlsx, стр. Z / раздел 'Ноутбуки'").
Предоставить пользователю возможность вручную инициировать переиндексацию всех файлов или содержимого конкретной папки на Яндекс.Диске.
Улучшить управление историей чатов: поиск по истории, возможность удаления отдельных чатов.
Цель этапа: Обеспечить стабильную, быструю и масштабируемую работу системы при увеличении объемов обрабатываемых данных. Существенно улучшить общий пользовательский опыт взаимодействия с приложением, сделав его более информативным и удобным.
Управление зависимостями (Composer)
Проект использует Composer для управления PHP-зависимостями и автозагрузкой классов. Основные команды (выполняются из корневой директории проекта):

Установка зависимостей:
bash
CopyInsert in Terminal
php composer.phar install
Обновление зависимостей:
bash
CopyInsert in Terminal
php composer.phar update
Перегенерация автозагрузчика (важно после добавления новых классов или изменения неймспейсов):
bash
CopyInsert in Terminal
php composer.phar dump-autoload -o
Загрузка классов (PSR-4)
Проект сконфигурирован для использования стандарта автозагрузки PSR-4. Неймспейс ResearcherAI\ соответствует директории classes/, как указано в composer.json:

json
CopyInsert
// composer.json (фрагмент)
"autoload": {
    "psr-4": {
        "ResearcherAI\\": "classes/"
    }
}
Все новые PHP классы должны размещаться в директории classes/ (или ее поддиректориях) и использовать неймспейс ResearcherAI (например, ResearcherAI\Helpers\SomeHelper для файла classes/Helpers/SomeHelper.php). После добавления или изменения структуры классов не забывайте выполнять команду php composer.phar dump-autoload -o.

## Поддержка

- Документация: `/docs/`
- Логи системы: `/logs/`
- GitHub Issues: [создать issue]

## Лицензия

Проприетарное ПО для kp-opt.ru

---

© 2024 Исследователь AI. Все права защищены.




Этап 1: Создание векторной базы и базовый RAG

Парсинг и Векторизация:
Используем существующий механизм извлечения текста из файлов.
Внедряем векторизацию этого текста с помощью доступных embedding-моделей (OpenAI или DeepSeek).
Разрабатываем новую структуру таблиц в cache.sqlite для хранения векторов, метаданных файлов (путь, хэш, дата изменения) и, возможно, отдельных текстовых "чанков" (фрагментов), если файлы большие.
Отслеживание изменений файлов:
Модифицируем YandexDiskClient и CacheManager для отслеживания изменений файлов (новые, измененные, удаленные).
При изменениях запускаем пере-векторизацию и обновление записей в БД.
Базовый RAG (Retrieval Augmented Generation):
Запрос пользователя векторизуется.
Выполняется поиск наиболее релевантных векторов (и соответствующих им текстовых фрагментов) в нашей SQLite базе.
Найденные фрагменты (контекст) вместе с исходным запросом передаются в GPT (OpenAI/DeepSeek) для генерации ответа.
Цель этапа: Получить работающую систему семантического поиска с актуализацией данных по файлам.
Этап 2: Улучшенный парсинг с GPT и отслеживание структуры папок

GPT-парсинг:
Заменяем простой парсинг текста на интеллектуальный парсинг с помощью GPT. Модель будет анализировать содержимое файла и извлекать структурированную информацию (товары, цены, характеристики).
Векторизоваться будут уже эти структурированные данные или осмысленные фрагменты, сгенерированные GPT. Это должно повысить качество векторов.
Отслеживание изменений структуры папок:
Дорабатываем YandexDiskClient для рекурсивного сканирования папок, отслеживания добавления/удаления не только файлов, но и вложенных папок.
Соответственно, обновляем логику в CacheManager (или новом VectorStoreManager) для корректной обработки изменений в иерархии папок и связанных с ними файлов/векторов.
Цель этапа: Значительно повысить качество извлекаемых данных для векторизации и обеспечить полную синхронизацию с любой структурой папок на Яндекс.Диске.
Этап 3: Оптимизация, масштабирование и UI/UX улучшения

Оптимизация хранения и поиска векторов:
Анализируем производительность. Если SQLite с простым поиском становится узким местом, внедряем расширения типа sqlite-vss для эффективного векторного поиска.
Как альтернатива (при очень больших объемах): рассматриваем миграцию векторных данных в специализированную векторную базу данных (например, Pinecone, Weaviate, Qdrant – но это усложнит архитектуру).
Фоновая обработка/очереди:
Для объемных прайс-листов процесс парсинга GPT и векторизации может быть долгим. Рассматриваем вынесение этих задач в фоновый режим (например, с использованием очередей задач), чтобы API отвечало быстрее, а индексация происходила асинхронно.
Улучшения UI/UX:
Возможно, добавить в интерфейс индикацию процесса индексации.
Улучшить отображение источников, если GPT-парсинг позволит точнее связывать ответ с конкретными частями прайс-листов.
Цель этапа: Обеспечить стабильную, быструю и масштабируемую работу системы, а также улучшить пользовательский опыт.


использовать только SQLite
всегда после доработах делать комит и пуш
ни каких локальных вариантов т.к. все тесты проходят на сервере